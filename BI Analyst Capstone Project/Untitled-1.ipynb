{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8f0ed3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maste\\AppData\\Local\\Temp\\ipykernel_6464\\2997253402.py:79: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df2[c] = pd.to_datetime(df2[c], errors=\"coerce\", infer_datetime_format=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'workbook': 'C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\BI_Analyst_Capstone_ML.xlsx',\n",
       " 'sheet_list_image': 'C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\Workbook_Sheet_Names.png',\n",
       " 'missing_images': ['C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\sales_missing_before.png',\n",
       "  'C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\sales_missing_after.png',\n",
       "  'C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\product_hierarchy_missing_before.png',\n",
       "  'C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\product_hierarchy_missing_after.png',\n",
       "  'C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\store_cities_missing_before.png',\n",
       "  'C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\store_cities_missing_after.png',\n",
       "  'C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\store_names_missing_before.png',\n",
       "  'C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\store_names_missing_after.png',\n",
       "  'C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\city_names_missing_before.png',\n",
       "  'C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\city_names_missing_after.png',\n",
       "  'C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\product_names_missing_before.png',\n",
       "  'C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\product_names_missing_after.png'],\n",
       " 'duplicate_images': ['C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\sales_duplicates_before.png',\n",
       "  'C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\sales_duplicates_after.png',\n",
       "  'C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\product_hierarchy_duplicates_before.png',\n",
       "  'C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\product_hierarchy_duplicates_after.png',\n",
       "  'C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\store_cities_duplicates_before.png',\n",
       "  'C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\store_cities_duplicates_after.png',\n",
       "  'C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\store_names_duplicates_before.png',\n",
       "  'C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\store_names_duplicates_after.png',\n",
       "  'C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\city_names_duplicates_before.png',\n",
       "  'C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\city_names_duplicates_after.png',\n",
       "  'C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\product_names_duplicates_before.png',\n",
       "  'C:\\\\Users\\\\maste\\\\Downloads\\\\BI Analyst Capstone Project\\\\New folder\\\\product_names_duplicates_after.png']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retry: Build Excel workbook and images again if previous attempt was interrupted\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import numbers\n",
    "from datetime import datetime\n",
    "# from ace_tools import display_dataframe_to_user\n",
    "\n",
    "DATA_DIR = r\"C:\\Users\\maste\\Downloads\\BI Analyst Capstone Project\\New folder\"\n",
    "INPUTS = [\n",
    "    \"sales.csv\",\n",
    "    \"product_hierarchy.csv\",\n",
    "    \"store_cities.csv\",\n",
    "    \"store_names.csv\",\n",
    "    \"city_names.csv\",\n",
    "    \"product_names.csv\",\n",
    "]\n",
    "\n",
    "OUTPUT_XLSX = os.path.join(DATA_DIR, \"BI_Analyst_Capstone_ML.xlsx\")\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def load_csv(path):\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=\"latin-1\")\n",
    "        except Exception as e2:\n",
    "            raise e2\n",
    "\n",
    "def detect_date_cols(df: pd.DataFrame):\n",
    "    cols = []\n",
    "    for c in df.columns:\n",
    "        cl = c.lower()\n",
    "        if any(k in cl for k in [\"date\", \"time\", \"timestamp\", \"datetime\"]):\n",
    "            cols.append(c)\n",
    "    return cols\n",
    "\n",
    "NUMERIC_STRIP_RE = re.compile(r\"[^\\d\\-\\.\\,]\")\n",
    "\n",
    "def numericify_object_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df2 = df.copy()\n",
    "    for c in df2.columns:\n",
    "        if pd.api.types.is_object_dtype(df2[c]):\n",
    "            s = df2[c].astype(str)\n",
    "            s = s.replace(r\"^\\s*$\", np.nan, regex=True)\n",
    "            mask = s.notna()\n",
    "            cleaned = s[mask].str.replace(r\"\\s+\", \"\", regex=True)\n",
    "            cleaned = cleaned.apply(lambda x: NUMERIC_STRIP_RE.sub(\"\", x))\n",
    "            def to_number(x):\n",
    "                if x.count(\",\") > 0 and x.count(\".\") > 0:\n",
    "                    return x.replace(\",\", \"\")\n",
    "                elif x.count(\",\") > 0 and x.count(\".\") == 0:\n",
    "                    return x.replace(\",\", \".\")\n",
    "                else:\n",
    "                    return x\n",
    "            cleaned = cleaned.apply(to_number)\n",
    "            converted = pd.to_numeric(cleaned, errors=\"coerce\")\n",
    "            if (converted.notna().sum() / len(df2)) > 0.3:\n",
    "                df2.loc[mask, c] = converted\n",
    "    return df2\n",
    "\n",
    "def trim_text(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df2 = df.copy()\n",
    "    for c in df2.columns:\n",
    "        if pd.api.types.is_object_dtype(df2[c]):\n",
    "            df2[c] = df2[c].astype(str).str.strip()\n",
    "            df2[c] = df2[c].replace(r\"^\\s*$\", np.nan, regex=True)\n",
    "    return df2\n",
    "\n",
    "def coerce_dates(df: pd.DataFrame, date_cols):\n",
    "    df2 = df.copy()\n",
    "    for c in date_cols:\n",
    "        try:\n",
    "            df2[c] = pd.to_datetime(df2[c], errors=\"coerce\", infer_datetime_format=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return df2\n",
    "\n",
    "def missing_counts(df: pd.DataFrame):\n",
    "    return df.isna().sum().sort_values(ascending=False)\n",
    "\n",
    "def plot_missing(title, series, out_path):\n",
    "    plt.figure()\n",
    "    if series.sum() == 0:\n",
    "        plt.text(0.5, 0.5, \"No missing values\", ha=\"center\", va=\"center\")\n",
    "        plt.title(title)\n",
    "        plt.axis(\"off\")\n",
    "    else:\n",
    "        series = series[series > 0]\n",
    "        series.plot(kind=\"bar\")\n",
    "        plt.title(title)\n",
    "        plt.ylabel(\"Missing Count\")\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=180, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_duplicates(title, count_before, sample_df, out_path):\n",
    "    plt.figure()\n",
    "    if count_before == 0 or sample_df is None or sample_df.empty:\n",
    "        plt.text(0.5, 0.5, \"No duplicates\", ha=\"center\", va=\"center\")\n",
    "        plt.title(title)\n",
    "        plt.axis(\"off\")\n",
    "    else:\n",
    "        n = min(10, len(sample_df))\n",
    "        tbl = plt.table(cellText=sample_df.head(n).astype(str).values,\n",
    "                        colLabels=sample_df.columns.astype(str),\n",
    "                        loc=\"center\")\n",
    "        tbl.auto_set_font_size(False)\n",
    "        tbl.set_fontsize(6)\n",
    "        tbl.scale(1, 1.2)\n",
    "        plt.title(title)\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "# ---------------- Load all inputs ----------------\n",
    "datasets = {}\n",
    "for fname in INPUTS:\n",
    "    fpath = os.path.join(DATA_DIR, fname)\n",
    "    if os.path.exists(fpath):\n",
    "        df = load_csv(fpath)\n",
    "        datasets[fname] = df\n",
    "        # display_dataframe_to_user(f\"preview_{fname}\", df.head(20))\n",
    "    else:\n",
    "        datasets[fname] = None\n",
    "\n",
    "# ---------------- Clean each dataset ----------------\n",
    "cleaned = {}\n",
    "summaries = {}\n",
    "missing_pngs = []\n",
    "duplicate_pngs = []\n",
    "\n",
    "for fname, df in datasets.items():\n",
    "    if df is None:\n",
    "        continue\n",
    "\n",
    "    miss_before = missing_counts(df)\n",
    "    missing_before_png = os.path.join(DATA_DIR, f\"{fname.replace('.csv','')}_missing_before.png\")\n",
    "    plot_missing(f\"{fname} — Missing Before\", miss_before, missing_before_png)\n",
    "    missing_pngs.append(missing_before_png)\n",
    "\n",
    "    dup_mask = df.duplicated()\n",
    "    dup_count_before = int(dup_mask.sum())\n",
    "    dup_sample = df[dup_mask]\n",
    "    duplicates_before_png = os.path.join(DATA_DIR, f\"{fname.replace('.csv','')}_duplicates_before.png\")\n",
    "    plot_duplicates(f\"{fname} — Duplicates Before: {dup_count_before}\", dup_count_before, dup_sample, duplicates_before_png)\n",
    "    duplicate_pngs.append(duplicates_before_png)\n",
    "\n",
    "    df1 = trim_text(df)\n",
    "    df2 = numericify_object_columns(df1)\n",
    "    date_cols = detect_date_cols(df2)\n",
    "    df3 = coerce_dates(df2, date_cols)\n",
    "    df4 = df3.dropna(how=\"any\").copy()\n",
    "    df5 = df4.drop_duplicates().copy()\n",
    "\n",
    "    cleaned[fname] = df5\n",
    "\n",
    "    miss_after = missing_counts(df5)\n",
    "    missing_after_png = os.path.join(DATA_DIR, f\"{fname.replace('.csv','')}_missing_after.png\")\n",
    "    plot_missing(f\"{fname} — Missing After\", miss_after, missing_after_png)\n",
    "    missing_pngs.append(missing_after_png)\n",
    "\n",
    "    dup_mask_after = df5.duplicated()\n",
    "    dup_count_after = int(dup_mask_after.sum())\n",
    "    dup_sample_after = df5[dup_mask_after] if dup_count_after > 0 else pd.DataFrame()\n",
    "    duplicates_after_png = os.path.join(DATA_DIR, f\"{fname.replace('.csv','')}_duplicates_after.png\")\n",
    "    plot_duplicates(f\"{fname} — Duplicates After: {dup_count_after}\", dup_count_after, dup_sample_after, duplicates_after_png)\n",
    "    duplicate_pngs.append(duplicates_after_png)\n",
    "\n",
    "    summaries[fname] = {\n",
    "        \"rows_before\": len(df),\n",
    "        \"rows_after\": len(df5),\n",
    "        \"columns\": len(df.columns),\n",
    "        \"missing_before_total\": int(df.isna().sum().sum()),\n",
    "        \"missing_after_total\": int(df5.isna().sum().sum()),\n",
    "        \"duplicates_before\": int(df.duplicated().sum()),\n",
    "        \"duplicates_after\": int(df5.duplicated().sum())\n",
    "    }\n",
    "\n",
    "# ---------------- Write Excel workbook ----------------\n",
    "with pd.ExcelWriter(OUTPUT_XLSX, engine=\"openpyxl\") as writer:\n",
    "    for fname, df in datasets.items():\n",
    "        if df is None:\n",
    "            continue\n",
    "        base = fname.replace(\".csv\", \"\")\n",
    "        df.head(50000).to_excel(writer, sheet_name=f\"{base}_raw\", index=False)\n",
    "        cleaned[fname].to_excel(writer, sheet_name=f\"{base}_cleaned\", index=False)\n",
    "\n",
    "    if summaries:\n",
    "        summ_tbl = pd.DataFrame.from_dict(summaries, orient=\"index\")\n",
    "        summ_tbl.index.name = \"dataset\"\n",
    "        summ_tbl.reset_index(inplace=True)\n",
    "        summ_tbl.to_excel(writer, sheet_name=\"cleaning_summary\", index=False)\n",
    "\n",
    "# Create workbook sheet names image\n",
    "try:\n",
    "    wb = load_workbook(OUTPUT_XLSX)\n",
    "    sheet_names = wb.sheetnames\n",
    "    txt = \"Workbook sheets:\\n\\n\" + \"\\n\".join(sheet_names)\n",
    "except Exception:\n",
    "    txt = \"Workbook not loaded\"\n",
    "\n",
    "sheet_names_img = os.path.join(DATA_DIR, \"Workbook_Sheet_Names.png\")\n",
    "plt.figure()\n",
    "plt.text(0.03, 0.97, txt, va=\"top\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(sheet_names_img, dpi=180, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# Show summary in UI\n",
    "try:\n",
    "    summary_df = pd.read_excel(OUTPUT_XLSX, sheet_name=\"cleaning_summary\")\n",
    "    # display_dataframe_to_user(\"cleaning_summary\", summary_df)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "{\n",
    "    \"workbook\": OUTPUT_XLSX,\n",
    "    \"sheet_list_image\": sheet_names_img,\n",
    "    \"missing_images\": missing_pngs,\n",
    "    \"duplicate_images\": duplicate_pngs\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
